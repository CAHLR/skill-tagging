{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab8c5d1",
   "metadata": {},
   "source": [
    "### 1. Fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8562076",
   "metadata": {},
   "source": [
    "References: \n",
    "- https://llama.meta.com/docs/how-to-guides/fine-tuning/\n",
    "- https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/tree/main/Fine-Tune%20Llama%202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286dc211",
   "metadata": {},
   "source": [
    "#### Login to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30003b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  #enter your tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d78015",
   "metadata": {},
   "source": [
    "#### QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d086e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "dataset_path = \"Path to your repo in the Hugging Face Hub where training and validation datasets are stored (e.g., myname/datasets).\"\n",
    "new_model = \"Your fine-tuned model name (e.g., llama-us-ccss)\"\n",
    "\n",
    "#QLORA\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "per_device_train_batch_size = 2\n",
    "per_device_eval_batch_size = 2\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 0\n",
    "logging_steps = 25\n",
    "\n",
    "max_seq_length = None\n",
    "packing = False\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad160b",
   "metadata": {},
   "source": [
    "#### File Uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1771898",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files_train = {\"train\": \"Your training file (e.g., data/examples/llama_finetuning_train.csv)\"}\n",
    "df_train = load_dataset(dataset_path, data_files=data_files_train, split=\"train\")\n",
    "\n",
    "data_files_val = {\"validation\": \"Your validation file (e.g., data/examples/llama_finetuning_val.csv)\"}\n",
    "df_val = load_dataset(dataset_path, data_files=data_files_val, split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f39043",
   "metadata": {},
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=df_train,\n",
    "    eval_dataset=df_val,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60046b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6aca9b",
   "metadata": {},
   "source": [
    "#### Store your new fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fe405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    #device_map=device_map,  # cuda out of memory\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c19c3",
   "metadata": {},
   "source": [
    "#### Push the model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f778cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "finetuned_model_path=\"Path to your repo in the Hugging Face Hub where your new fine-tuned model will be stored (e.g., myname/fine-tuning)\"\n",
    "\n",
    "model.push_to_hub(finetuned_model_path, check_pr=True)\n",
    "tokenizer.push_to_hub(finetuned_model_path,check_pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6a63c",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826723d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=finetuned_model_path\n",
    "hf_auth=\"Your tokens\"\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "device_name=torch.cuda.get_device_name(0)\n",
    "print(f\"Using device:{device} ({device_name})\")\n",
    "\n",
    "bnb_config=transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True,\n",
    "                                          bnb_4bit_compute_dtype=bfloat16)\n",
    "\n",
    "model_config=transformers.AutoConfig.from_pretrained(model_id, use_auth_token=hf_auth)\n",
    "\n",
    "model=transformers.AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                       trust_remote_code=True,\n",
    "                                                       config=model_config,\n",
    "                                                       quantization_config=bnb_config,\n",
    "                                                       use_auth_token=hf_auth)\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "tokenizer=transformers.AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e031f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"Path to an input file containing the problems\"  \n",
    "pred_csv_file_path=\"Path to an output file containing the problems and skill predictions\"\n",
    "\n",
    "problems = []\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        problems.append(row['problem'])  \n",
    "        \n",
    "generate_text=transformers.pipeline(model=model,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   return_full_text=True,\n",
    "                                   task='text-generation',\n",
    "                                   temperature=0.2,\n",
    "                                   max_new_tokens=250,\n",
    "                                   repetition_penalty=1.1)\n",
    "\n",
    "predicted_skills = []\n",
    "\n",
    "for problem_text in problems:\n",
    "    prompt = f\"\"\"\n",
    "Please provide one standard from the Common Core Standards for Mathematics that is most closely related to the given problem. Problem: \"{problem_text}\"\n",
    "\"\"\"\n",
    "    prompt_template=f'''[INST] <<SYS>>\n",
    "\n",
    "\n",
    "    {prompt} [/INST]\n",
    " \n",
    "    '''\n",
    "    res=generate_text(prompt)\n",
    "    output_text=res[0][\"generated_text\"]\n",
    "    \n",
    "      \n",
    "    predicted_skill = output_text\n",
    "    \n",
    "    if predicted_skill:\n",
    "        predicted_skills.append(predicted_skill)\n",
    "    else:\n",
    "        predicted_skills.append('')\n",
    "\n",
    "updated_rows = []\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    fieldnames = csv_reader.fieldnames + ['pred_skill']\n",
    "    for row, predicted_skill in zip(csv_reader, predicted_skills):\n",
    "        row['pred_skill'] = predicted_skill\n",
    "        updated_rows.append(row)\n",
    "\n",
    "with open(pred_csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(updated_rows)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7b624",
   "metadata": {},
   "source": [
    "### 2. Standards in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"meta-llama/Llama-2-13b-chat-hf\"\n",
    "hf_auth=\"Your tokens\"\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "device_name=torch.cuda.get_device_name(0)\n",
    "print(f\"Using device:{device} ({device_name})\")\n",
    "\n",
    "bnb_config=transformers.BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True,\n",
    "                                          bnb_4bit_compute_dtype=bfloat16)\n",
    "\n",
    "model_config=transformers.AutoConfig.from_pretrained(model_id, use_auth_token=hf_auth)\n",
    "\n",
    "model=transformers.AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                       trust_remote_code=True,\n",
    "                                                       config=model_config,\n",
    "                                                       quantization_config=bnb_config,\n",
    "                                                       use_auth_token=hf_auth)\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "tokenizer=transformers.AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7758d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"Path to an input file containing the problems\"  \n",
    "pred_csv_file_path=\"Path to an output file containing the problems and skill predictions\"\n",
    "standards=\"A list of standards (skills)\"\n",
    "\n",
    "problems = []\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        problems.append(row['problem'])  \n",
    "        \n",
    "generate_text=transformers.pipeline(model=model,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   return_full_text=True,\n",
    "                                   task='text-generation',\n",
    "                                   temperature=0.2,\n",
    "                                   max_new_tokens=250,\n",
    "                                   repetition_penalty=1.1)\n",
    "\n",
    "predicted_skills = []\n",
    "\n",
    "for problem_text in problems:\n",
    "    prompt = f\"\"\"\n",
    "Please provide one standard from the Common Core Standards for Mathematics that is most closely related to the given problem. Problem: \"{problem_text}\", Standards: {standards}\n",
    "\"\"\"\n",
    "    prompt_template=f'''[INST] <<SYS>>\n",
    "\n",
    "\n",
    "    {prompt} [/INST]\n",
    " \n",
    "    '''\n",
    "    res=generate_text(prompt)\n",
    "    output_text=res[0][\"generated_text\"]\n",
    "    \n",
    "    predicted_skill = output_text\n",
    "    \n",
    "    if predicted_skill:\n",
    "        predicted_skills.append(predicted_skill)\n",
    "    else:\n",
    "        predicted_skills.append('')\n",
    "\n",
    "updated_rows = []\n",
    "with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    fieldnames = csv_reader.fieldnames + ['pred_skill']\n",
    "    for row, predicted_skill in zip(csv_reader, predicted_skills):\n",
    "        row['pred_skill'] = predicted_skill\n",
    "        updated_rows.append(row)\n",
    "\n",
    "with open(pred_csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(updated_rows)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
